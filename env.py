import torch
import torch.nn as nn
import intel_extension_for_pytorch as ipex

# 1ï¸âƒ£ è®¾å¤‡é€‰æ‹©
device = torch.device("xpu" if torch.xpu.is_available() else "cpu")
print(f"ğŸ”¥ Current Device: {device}")

# 2ï¸âƒ£ æ¨¡å‹ï¼šæ¢ä¸€ä¸ªâ€œå¤§å‹åŠ›â€æ¨¡å‹ (3å±‚ 4096 å®½åº¦çš„ MLPï¼Œæ¨¡æ‹Ÿé«˜è´Ÿè½½çŸ©é˜µä¹˜æ³•)
class HeavyPressureModel(nn.Module):
    def __init__(self):
        super().__init__()
        # å¢å¤§ç»´åº¦ï¼šä» 1024 -> 4096ï¼Œè®¡ç®—é‡ç¿» 16 å€
        self.fc1 = nn.Linear(4096, 4096)
        self.act1 = nn.GELU()  # GELU æ¯” ReLU è®¡ç®—ç¨å¾®é‡ä¸€ç‚¹
        
        self.fc2 = nn.Linear(4096, 4096)
        self.act2 = nn.GELU()
        
        self.fc3 = nn.Linear(4096, 4096) # "ä¸‰" å‹åŠ› -> ç¬¬ä¸‰å±‚

    def forward(self, x):
        x = self.act1(self.fc1(x))
        x = self.act2(self.fc2(x))
        x = self.fc3(x)
        return x

model = HeavyPressureModel().to(device)
model.eval()

# 3ï¸âƒ£ æ•°æ®ï¼šå¢å¤§ Batch Size (å¢åŠ ååå‹åŠ›)
# 4096ç»´ * 64 batch size = å¾ˆå¤§çš„çŸ©é˜µ
BATCH_SIZE = 64
input_data = torch.randn(BATCH_SIZE, 4096, device=device)

# 4ï¸âƒ£ IPEX ä¼˜åŒ–ï¼šå¼€å¯ BF16 (BFloat16)
# Intel ç¡¬ä»¶(CPU/Arc/Data Center GPU) è·‘ BF16 æ•ˆç‡æœ€é«˜ï¼Œå‹åŠ›æµ‹è¯•å¿…å¼€
print("ğŸ› ï¸  Optimizing with IPEX (BF16)...")
try:
    # å°è¯•å¼€å¯ BF16 ä¼˜åŒ–
    model = ipex.optimize(model, dtype=torch.bfloat16)
    use_bf16 = True
    print("âœ… BF16 Optimization Enabled.")
except Exception as e:
    # å¦‚æœç¡¬ä»¶ä¸æ”¯æŒ BF16ï¼Œå›é€€åˆ° FP32
    print(f"âš ï¸  BF16 not supported ({e}), fallback to FP32.")
    model = ipex.optimize(model)
    use_bf16 = False

# 5ï¸âƒ£ æ¨ç† (Forward)
print("ğŸš€ Running Forward Pass (Stress Test)...")

# æ ¹æ®è®¾å¤‡ç±»å‹é€‰æ‹© AMP ä¸Šä¸‹æ–‡
amp_device_type = "xpu" if device.type == "xpu" else "cpu"

with torch.no_grad():
    # é¢„çƒ­ (Warmup) - è®©ç¡¬ä»¶è¿›å…¥é«˜æ€§èƒ½çŠ¶æ€
    for _ in range(5):
        if use_bf16:
            with torch.autocast(device_type=amp_device_type, enabled=True, dtype=torch.bfloat16):
                _ = model(input_data)
        else:
            _ = model(input_data)
            
    # æ­£å¼è¿è¡Œ
    import time
    start = time.time()
    
    if use_bf16:
        with torch.autocast(device_type=amp_device_type, enabled=True, dtype=torch.bfloat16):
            output = model(input_data)
    else:
        output = model(input_data)
        
    cost = time.time() - start

print(f"âœ… Forward OK. Output shape: {output.shape}")
print(f"â±ï¸  Time cost: {cost * 1000:.2f} ms")