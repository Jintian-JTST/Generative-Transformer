{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c942c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê≠£Âú®Ë°•ÂÖÖÂÆâË£ÖÁº∫Â§±ÁöÑËøêË°åÂ∫ì...\n",
      "Requirement already satisfied: intel-openmp in d:\\users\\jtst\\desktop\\desktop\\jtst\\.tech\\generative-transformer\\ai\\lib\\site-packages (2025.3.2)\n",
      "Requirement already satisfied: mkl in d:\\users\\jtst\\desktop\\desktop\\jtst\\.tech\\generative-transformer\\ai\\lib\\site-packages (2025.3.1)\n",
      "Requirement already satisfied: mkl-include in d:\\users\\jtst\\desktop\\desktop\\jtst\\.tech\\generative-transformer\\ai\\lib\\site-packages (2025.3.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in d:\\users\\jtst\\desktop\\desktop\\jtst\\.tech\\generative-transformer\\ai\\lib\\site-packages (2025.3.2)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur in d:\\users\\jtst\\desktop\\desktop\\jtst\\.tech\\generative-transformer\\ai\\lib\\site-packages (2025.3.2)\n",
      "Requirement already satisfied: numpy in d:\\users\\jtst\\desktop\\desktop\\jtst\\.tech\\generative-transformer\\ai\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: umf==1.0.* in d:\\users\\jtst\\desktop\\desktop\\jtst\\.tech\\generative-transformer\\ai\\lib\\site-packages (from intel-cmplr-lib-ur) (1.0.3)\n",
      "Requirement already satisfied: tcmlib>=1.4 in d:\\users\\jtst\\desktop\\desktop\\jtst\\.tech\\generative-transformer\\ai\\lib\\site-packages (from umf==1.0.*->intel-cmplr-lib-ur) (1.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.1 in d:\\users\\jtst\\desktop\\desktop\\jtst\\.tech\\generative-transformer\\ai\\lib\\site-packages (from mkl) (2025.3.1)\n",
      "Requirement already satisfied: tbb==2022.* in d:\\users\\jtst\\desktop\\desktop\\jtst\\.tech\\generative-transformer\\ai\\lib\\site-packages (from mkl) (2022.3.1)\n",
      "ËøêË°åÂ∫ìÂÆâË£ÖÂÆåÊàê„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import site\n",
    "\n",
    "# 1. Á°ÆÂÆöÊêúÁ¥¢Ê†πÁõÆÂΩï\n",
    "venv_root = Path(sys.prefix)\n",
    "site_packages = venv_root / \"Lib\" / \"site-packages\"\n",
    "print(f\"üïµÔ∏è Ê≠£Âú®Âú∞ÊØØÂºèÊêúÂØª DLLÔºåÁõÆÊ†áÂå∫Âüü: {site_packages}\")\n",
    "\n",
    "# 2. ÂÆö‰πâÂøÖÈ°ªË¶ÅÊâæÂà∞ÁöÑÂÖ≥ÈîÆ DLL Êñá‰ª∂ÂêçÁâπÂæÅ\n",
    "# IPEX ÂêØÂä®Ëá≥Â∞ëÈúÄË¶ÅËøô‰∏§‰∏™Ê†∏ÂøÉÂ∫ì\n",
    "critical_dlls = [\n",
    "    \"sycl*.dll\",        # ÁºñËØëÂô®ËøêË°åÊó∂ (Compiler Runtime)\n",
    "    \"pi_level_zero.dll\",# GPU Êé•Âè£ (Level Zero)\n",
    "    \"mkl_core.dll\",     # Êï∞Â≠¶Â∫ìÊ†∏ÂøÉ\n",
    "    \"iomp5md.dll\"       # OpenMP Â∫ì\n",
    "]\n",
    "\n",
    "found_paths = set()\n",
    "\n",
    "# 3. ÈÄíÂΩíÊêúÁ¥¢ (Êó†ËÆ∫ÂÆÉËóèÂú® bin ËøòÊòØ lib ËøòÊòØÂÖ∂‰ªñÂú∞Êñπ)\n",
    "# ËøôÊ≠•ÂèØËÉΩÈúÄË¶ÅÂá†ÁßíÈíü\n",
    "for pattern in critical_dlls:\n",
    "    # rglob = ÈÄíÂΩíÊü•Êâæ\n",
    "    for dll_path in site_packages.rglob(pattern):\n",
    "        # Ëé∑ÂèñËØ• DLL ÊâÄÂú®ÁöÑÊñá‰ª∂Â§π\n",
    "        parent_dir = dll_path.parent\n",
    "        if parent_dir not in found_paths:\n",
    "            found_paths.add(parent_dir)\n",
    "            try:\n",
    "                os.add_dll_directory(str(parent_dir))\n",
    "                os.environ['PATH'] = str(parent_dir) + ';' + os.environ['PATH']\n",
    "                print(f\"[‚úÖ ÊâæÂà∞Âπ∂Âä†ËΩΩ] {dll_path.name} -> {parent_dir.relative_to(site_packages)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[‚ùå Âä†ËΩΩÂá∫Èîô] {parent_dir}: {e}\")\n",
    "\n",
    "if not found_paths:\n",
    "    print(\"\\n[üíÄ Ëá¥ÂëΩÈîôËØØ] Ê†πÊú¨Êâæ‰∏çÂà∞ sycl*.dll Êàñ mkl*.dll„ÄÇ\")\n",
    "    print(\"ËøôÊÑèÂë≥ÁùÄÊñá‰ª∂ÁúüÁöÑ‰∏çÂú®Á°¨Áõò‰∏ä„ÄÇËØ∑ÊâßË°å‰∏ãÊñπÁöÑ„ÄêÈôçÁ∫ßÂÆâË£ÖÊñπÊ°à„Äë„ÄÇ\")\n",
    "else:\n",
    "    print(f\"\\nÂÖ±Ëá™Âä®Ê≥®ÂÜå‰∫Ü {len(found_paths)} ‰∏™ DLL Ë∑ØÂæÑ„ÄÇ\")\n",
    "\n",
    "# 4. Â∞ùËØïÂØºÂÖ•\n",
    "print(\"\\nü§û Ê≠£Âú®Â∞ùËØïÂØºÂÖ• PyTorch...\")\n",
    "try:\n",
    "    import torch\n",
    "    import intel_extension_for_pytorch as ipex\n",
    "    \n",
    "    print(\"\\nüöÄüöÄüöÄ ÊàêÂäü‰∫ÜÔºÅ\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    print(f\"IPEX: {ipex.__version__}\")\n",
    "    \n",
    "    if torch.xpu.is_available():\n",
    "        print(f\"ÊòæÂç°: {torch.xpu.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"Ë≠¶Âëä: ‰πüÂ∞±ÊòØÊ≤°Ê£ÄÊµãÂà∞ÊòæÂç°Ôºå‰ΩÜÂ∫ìÂ∑≤ÁªèËÉΩÁî®‰∫Ü„ÄÇ\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå ÂØºÂÖ•‰æùÁÑ∂Â§±Ë¥•: {e}\")\n",
    "except OSError as e:\n",
    "    print(f\"\\n‚ùå ‰æùÁÑ∂Êä• DLL Áº∫Â§± (WinError 126): {e}\")\n",
    "    print(\"ËøôÈÄöÂ∏∏ÊÑèÂë≥ÁùÄ 2025 ÁâàÁöÑ Runtime ‰∏é 2.1 ÁâàÁöÑ IPEX ‰∏çÂÖºÂÆπ„ÄÇ\")\n",
    "    print(\"ËØ∑ÊâßË°å‰∏ãÊñπÁöÑ„ÄêÈôçÁ∫ßÂÆâË£ÖÊñπÊ°à„Äë„ÄÇ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0b9656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 82\n",
      "Characters: \n",
      " !'()*,-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz‚Äì‚Äî‚Äò‚Äô‚Äú‚Äù‚Ä¶\n",
      "[56, 53, 60, 60, 63, 1, 71, 63, 66, 60, 52]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "with open(r\"dataset/01.txt\", \"r\", encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "'''print(len(content))\n",
    "print(content[:100])'''  # Print the first 100 characters to verify content\n",
    "\n",
    "chars=sorted(list(set(content)))\n",
    "vocab_size=len(chars)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Characters:\", ''.join(chars))\n",
    "\n",
    "# Create mappings from characters to integers and vice versa\n",
    "stoi={ch:i for i,ch in enumerate(chars)}\n",
    "itos={i:ch for i,ch in enumerate(chars)}\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "def decode(l):\n",
    "    return ''.join(itos[i] for i in l)\n",
    "\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))\n",
    "\n",
    "devices = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffb671e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] Êâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊ®°Âùó„ÄÇ Error loading \"d:\\Users\\JTST\\Desktop\\Desktop\\JTST\\.TECH\\Generative-Transformer\\ai\\Lib\\site-packages\\torch\\lib\\backend_with_compiler.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m data = torch.tensor(encode(content), dtype=torch.long)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(data.shape, data.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\JTST\\Desktop\\Desktop\\JTST\\.TECH\\Generative-Transformer\\ai\\Lib\\site-packages\\torch\\__init__.py:139\u001b[39m\n\u001b[32m    137\u001b[39m                 err = ctypes.WinError(ctypes.get_last_error())\n\u001b[32m    138\u001b[39m                 err.strerror += \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    141\u001b[39m     kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[31mOSError\u001b[39m: [WinError 126] Êâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊ®°Âùó„ÄÇ Error loading \"d:\\Users\\JTST\\Desktop\\Desktop\\JTST\\.TECH\\Generative-Transformer\\ai\\Lib\\site-packages\\torch\\lib\\backend_with_compiler.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(content), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])  # Print the first 100 encoded integers to verify encoding\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Demonstrate how to create input-target pairs for training\n",
    "block_size = 8  # context length for predictions\n",
    "train_data[:block_size+1]\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context.tolist()} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877290b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "batch_size = 4\n",
    "x, y = get_batch('train')\n",
    "print(\"inputs:\")\n",
    "print(x)\n",
    "print(\"targets:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d79892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "    \n",
    "# ÂÅáËÆæ‰Ω†‰πãÂâçÂ∑≤ÁªèÂÆö‰πâ‰∫Ü vocab_size ÂíåËé∑Âèñ‰∫Ü x, y\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(x, y)\n",
    "print(\"logits shape:\", logits.shape)\n",
    "print(\"loss:\", loss)\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))  # Decode the predicted tokens for verification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f7ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "for steps in range(1000):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"step {steps+1}: loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))  # Decode the predicted tokens for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b93be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
